<1>
在Anna的1080Ti上训练的时候，如果将FB15K模型的注意力改为三头，会出现显存不够的问题。
$ CUDA_VISIBLE_DEVICES=1 python3 main.py --data ./data/FB15k-237/ --epochs_gat 3000 --epochs_conv 150 --weight_decay_gat 0.00001 --get_2hop True --partial_2hop True --batch_size_gat 272115 --margin 1 --out_channels 50 --drop_conv 0.3 --output_folder ./checkpoints/fb/out/

解决方法是减小batch size，我直接删掉了命令的参数中的batch size选项
$ CUDA_VISIBLE_DEVICES=1 python3 main.py --data ./data/FB15k-237/ --epochs_gat 3000 --epochs_conv 150 --weight_decay_gat 0.00001 --get_2hop True --partial_2hop True --margin 1 --out_channels 50 --drop_conv 0.3 --output_folder ./checkpoints/fb/out/
减小后就使用默认的batch大小了：

args.add_argument("-b_gat", "--batch_size_gat", type=int,
                      default=86835, help="Batch size for GAT")

